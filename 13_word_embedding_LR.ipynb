{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word embedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzJ4eOHLN0Zv",
        "colab_type": "text"
      },
      "source": [
        "## Preparing Data\n",
        "\n",
        "One of the main concepts of TorchText is the Field. These define how your data should be processed. In our sentiment classification task the data consists of both the raw string of the review and the sentiment, either \"pos\" or \"neg\".\n",
        "\n",
        "The parameters of a Field specify how the data should be processed.\n",
        "\n",
        "We use the TEXT field to define how the review should be processed, and the LABEL field to process the sentiment.\n",
        "\n",
        "Our TEXT field has tokenize='spacy' as an argument. This defines that the \"tokenization\" (the act of splitting the string into discrete \"tokens\") should be done using the [spaCy](https://spacy.io/) tokenizer. If no tokenize argument is passed, the default is simply splitting the string on spaces.\n",
        "\n",
        "LABEL is defined by a LabelField, a special subset of the Field class specifically used for handling labels. We will explain the dtype argument later.\n",
        "\n",
        "For more on TorchText, go [here](https://pytorch.org/text/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMfjm4tuNJuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy')\n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE_bZhQ4OXDH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Another handy feature of TorchText is that it has support for common datasets used in natural language processing (NLP).\n",
        "\n",
        "The following code automatically downloads the IMDb dataset and splits it into the canonical train/test splits as torchtext.datasets objects. It process the data using the Fields we have previously defined. The IMDb dataset consists of 50,000 movie reviews, each marked as being a positive or negative review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyYh7Rz6OUSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crmFOeXAOey0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bb476619-fcd6-4ce2-8ca5-8490feb70285"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 25000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEHbJS3lOreT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d1dd70b0-620c-4ff7-aed7-fdedb0ecdaa5"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'text': ['I', \"'ll\", 'tell', 'you', 'a', 'tale', 'of', 'the', 'summer', 'of', '1994', '.', 'A', 'friend', 'and', 'I', 'attended', 'a', 'Canada', 'Day', 'concert', 'in', 'Barrie', ',', 'and', 'it', 'was', 'a', 'who', \"'s\", 'who', 'of', 'the', 'top', 'Canadian', 'bands', 'of', 'the', 'age', '.', 'We', 'got', 'there', 'about', '4', 'am', ',', 'waited', 'in', 'line', 'most', 'of', 'the', 'morning', ',', 'and', 'when', 'the', 'doors', 'opened', 'at', '9', 'am', ',', 'we', 'were', 'among', 'the', 'first', 'inside', 'the', 'gates', '.', 'We', 'then', 'waited', 'and', 'waited', 'in', 'the', 'hot', 'sun', ',', 'slowly', 'broiling', 'but', 'we', 'did', \"n't\", 'care', ',', 'because', 'the', 'headliners', 'were', 'among', 'our', 'favourites', '.', 'At', 'one', 'point', ',', 'early', 'in', 'the', 'afternoon', ',', 'I', 'sat', 'down', 'and', 'dozed', 'off', 'with', 'my', 'back', 'to', 'the', 'barrier', '.', 'I', 'was', 'awakened', 'to', 'my', 'shock', 'and', 'dismay', 'by', 'a', 'shrieking', 'girl', 'wearing', 'a', 'Rheostatics', 't', '-', 'shirt', '.', 'This', 'is', 'the', 'reason', 'I', 'have', 'hated', 'the', 'Rheostatics', 'to', 'this', 'day', '.', 'There', \"'s\", 'nothing', 'reasonable', ',', 'nor', 'taste', '-', 'determined', ',', 'nor', 'really', 'anything', 'except', 'their', 'fandom', '.', 'Snotty', 'of', 'me', ',', 'is', \"n't\", 'it', '?', 'So', ',', 'I', ',', 'in', 'my', 'hatred', 'of', 'the', 'band', ',', 'have', 'denied', 'myself', 'the', 'delight', 'that', 'is', 'Whale', 'Music.<br', '/><br', '/>Desmond', 'Howl', 'had', 'it', 'all', '.', 'It', \"'s\", 'hard', 'to', 'say', 'what', 'he', \"'s\", 'lost', ',', 'since', 'he', 'lives', 'in', 'a', 'fantastic', 'mansion', 'wedged', 'between', 'the', 'ocean', 'and', 'the', 'mountains', '(', 'the', 'BC', 'region', 'where', 'the', 'movie', 'was', 'shot', 'is', 'breathtaking', ')', '.', 'The', 'life', 'most', 'of', 'us', 'dream', 'of', 'is', 'dismantled', 'by', 'dreams', ',', 'phantoms', ',', 'and', 'his', 'own', 'past', ',', 'until', 'the', 'day', 'a', 'teenaged', 'criminal', 'breaks', 'in', '...', 'and', ',', 'trite', 'as', 'it', 'sounds', ',', 'breaks', 'him', 'out.<br', '/><br', '/>Canadian', 'cinema', 'suffers', 'from', 'several', 'problems', '.', 'Generally', ',', 'a', 'lack', 'of', 'money', ',', 'as', 'well', 'as', 'an', 'insufferable', 'lack', 'of', 'asking', 'for', 'help', '(', 'as', 'if', 'somehow', 'the', 'feature', 'would', 'cease', 'to', 'be', 'Canadian', ')', 'leads', 'to', 'lower', 'production', 'values', 'than', 'American', 'or', 'British', 'films', ',', 'and', 'most', 'people', 'do', \"n't\", 'like', 'to', 'watch', 'anything', 'that', 'sounds', 'or', 'looks', 'like', ',', 'well', ',', 'not', 'like', 'an', 'American', 'film', '.', 'Next', ',', 'Canadian', 'screenwriters', 'often', 'seem', 'so', 'caught', 'up', 'in', 'being', 'weird', 'that', 'they', 'lose', 'sight', 'of', 'how', 'to', 'tell', 'a', 'good', 'story', ',', 'and', 'tell', 'it', 'well', '.', 'Third', ',', 'they', 'seem', 'to', 'think', 'that', 'gratuitous', 'nudity', '(', 'often', 'full', '-', 'frontal', ')', 'makes', 'something', 'artistic', '.', 'I', \"'m\", 'sure', 'anyone', 'who', 'watches', 'enough', 'Canadian', 'movies', ',', 'especially', 'late', 'at', 'night', 'on', 'the', 'CBC', ',', 'knows', 'exactly', 'what', 'I', \"'m\", 'talking', 'about', '.', 'It', \"'s\", 'almost', 'like', 'a', '\"', 'do', \"n't\", 'do', 'this', '\"', 'handbook', 'exists', 'out', 'there', 'somewhere', 'and', 'Canadian', 'film', '-', 'makers', 'threw', 'it', 'out', 'a', 'long', 'time', 'ago.<br', '/><br', '/>In', 'the', '90s', 'and', '00s', ',', 'however', ',', 'some', 'films', '(', 'such', 'as', 'Bruce', 'McDonald', \"'s\", 'work', 'and', 'the', 'brilliant', 'C.R.A.Z.Y.', ')', 'have', 'broken', 'this', 'mold', ',', 'and', 'managed', 'to', 'maintain', 'what', 'makes', 'them', 'Canadian', ',', 'while', 'holding', 'onto', 'watchable', 'production', 'values', 'and', 'great', 'stories', '.', 'Whale', 'Music', 'is', 'such', 'a', 'film', ',', 'on', 'the', 'surface', '.', 'Deeper', 'than', 'just', 'its', 'Canadian', '-', 'isms', ',', 'it', \"'s\", 'a', 'deeply', 'moving', 'story', 'of', 'a', 'man', 'who', \"'s\", 'lost', 'his', 'grip', ',', 'through', 'grief', 'and', 'excess', ',', 'who', 'is', 'redeemed', 'by', 'music', 'then', 'by', 'love', '.', 'And', 'that', 'redeems', 'even', 'the', 'Rheostatics', '.', ':)'], 'label': 'pos'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlghldduOt1c",
        "colab_type": "text"
      },
      "source": [
        "The IMDb dataset only has train/test splits, so we need to create a validation set. We can do this with the .split() method.\n",
        "\n",
        "By default this splits 70/30, however by passing a split_ratio argument, we can change the ratio of the split, i.e. a split_ratio of 0.8 would mean 80% of the examples make up the training set and 20% make up the validation set.\n",
        "\n",
        "We also pass our random seed to the random_state argument, ensuring that we get the same train/validation split each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwBDLQHWOwPX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR5HDd5DPGNm",
        "colab_type": "text"
      },
      "source": [
        "Again, we'll view how many examples are in each split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVSBt3NzPIXn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "cb50c95e-2033-4b75-8a37-b0d4de36fe71"
      },
      "source": [
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 17500\n",
            "Number of validation examples: 7500\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9-FvaVSPasb",
        "colab_type": "text"
      },
      "source": [
        "The following builds the vocabulary, only keeping the most common max_size tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_75hqlzDPbhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffiCWZXtPpuL",
        "colab_type": "text"
      },
      "source": [
        "Why do we only build the vocabulary on the training set? When testing any machine learning system you do not want to look at the test set in any way. We do not include the validation set as we want it to reflect the test set as much as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFP0HD5IP5ZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "600f18d1-d2ac-413a-927b-ad6722935fae"
      },
      "source": [
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 25002\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTVm8i1FP8Kr",
        "colab_type": "text"
      },
      "source": [
        "We can also view the most common words in the vocabulary and their frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm1j-eGrP-mm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3ef97031-f3e8-4c3d-9162-1e317997a005"
      },
      "source": [
        "print(TEXT.vocab.freqs.most_common(20))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('the', 202740), (',', 193041), ('.', 165817), ('and', 109865), ('a', 109697), ('of', 100794), ('to', 94173), ('is', 76159), ('in', 61417), ('I', 54936), ('it', 53936), ('that', 49515), ('\"', 44192), (\"'s\", 43416), ('this', 42703), ('-', 37456), ('/><br', 35832), ('was', 35209), ('as', 30609), ('with', 30105)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ6Ltyd1QDyu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5770d7df-5580-4101-c2bf-10b69f65bdb7"
      },
      "source": [
        "print(LABEL.vocab.stoi)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<function _default_unk_index at 0x7f0e1f5c4158>, {'neg': 0, 'pos': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRoIvqFVQgrB",
        "colab_type": "text"
      },
      "source": [
        "## Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip629LPWQiXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js_SU_DwSHkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "18a2be6c-2a7e-4963-dcb1-1711cb9a5eba"
      },
      "source": [
        "embeds = nn.Embedding(len(TEXT.vocab), 10)  # number of words in vocab, 10 dimensional embeddings\n",
        "lookup_tensor = torch.tensor(TEXT.vocab.stoi['hello'], dtype=torch.long)\n",
        "hello_embed = embeds(lookup_tensor)\n",
        "print(hello_embed)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.5849, -0.0740, -1.8753, -0.3271, -0.3832, -0.6796, -0.3518, -0.4548,\n",
            "         0.4912,  2.0177], grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NRSu3yLTBch",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtipZtTyTT90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LR(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        \n",
        "        # self.flatten = nn.Flatten()\n",
        "        \n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "       \n",
        "        embedded = self.embedding(text)     \n",
        "\n",
        "        hidden = torch.mean(embedded, dim=0)\n",
        "        \n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4C48EzFyWXGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqHwmJQqXkSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = LR(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnhaWXkCY3_u",
        "colab_type": "text"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAnU5cwOY5W_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZtqVlTdY9Sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWaPA92dZKOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        predictions = model(batch.text).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXw3vGHLZP1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = model(batch.text).squeeze(1)\n",
        "            \n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_fWGPi-ZU_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-LXcCzSZYFZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "c0b8ae10-394d-4177-a4a5-b98ab7286ec2"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'lr-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.691 | Train Acc: 52.23%\n",
            "\t Val. Loss: 0.666 |  Val. Acc: 70.44%\n",
            "Epoch: 02 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.679 | Train Acc: 61.34%\n",
            "\t Val. Loss: 0.617 |  Val. Acc: 74.74%\n",
            "Epoch: 03 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.657 | Train Acc: 70.44%\n",
            "\t Val. Loss: 0.547 |  Val. Acc: 76.30%\n",
            "Epoch: 04 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.621 | Train Acc: 75.02%\n",
            "\t Val. Loss: 0.493 |  Val. Acc: 77.57%\n",
            "Epoch: 05 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.580 | Train Acc: 78.48%\n",
            "\t Val. Loss: 0.465 |  Val. Acc: 78.94%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4yDUDvucV_D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8862ada0-8fdc-43f8-bb65-6632c2150cdb"
      },
      "source": [
        "model.load_state_dict(torch.load('lr-model.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.484 | Test Acc: 77.21%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55_7-P3Lc6Gb",
        "colab_type": "text"
      },
      "source": [
        "## Use pre-trained word embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LAKKOMPdNl8",
        "colab_type": "text"
      },
      "source": [
        "Next is the use of pre-trained word embeddings. Now, instead of having our word embeddings initialized randomly, they are initialized with these pre-trained vectors. We get these vectors simply by specifying which vectors we want and passing it as an argument to build_vocab. TorchText handles downloading the vectors and associating them with the correct words in our vocabulary.\n",
        "\n",
        "Here, we'll be using the \"glove.6B.100d\" vectors\". glove is the algorithm used to calculate the vectors, go [here](https://nlp.stanford.edu/projects/glove/) for more. 6B indicates these vectors were trained on 6 billion tokens and 100d indicates these vectors are 100-dimensional.\n",
        "\n",
        "You can see the other available vectors [here](https://github.com/pytorch/text/blob/master/torchtext/vocab.py#L113).\n",
        "\n",
        "The theory is that these pre-trained vectors already have words with similar semantic meaning close together in vector space, e.g. \"terrible\", \"awful\", \"dreadful\" are nearby. This gives our embedding layer a good initialization as it does not have to learn these relations from scratch.\n",
        "\n",
        "Note: these vectors are about 862MB, so watch out if you have a limited internet connection.\n",
        "\n",
        "By default, TorchText will initialize words in your vocabulary but not in your pre-trained embeddings to zero. We don't want this, and instead initialize them randomly by setting unk_init to torch.Tensor.normal_. This will now initialize those words via a Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t65f5uvHc120",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB_SIZE = 25000\n",
        "\n",
        "# TEXT = data.Field(tokenize = 'spacy')\n",
        "# LABEL = data.LabelField(dtype = torch.float)\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxqd4bOTe2FY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "35c6cb51-8845-4cb8-c88d-11b17808f50d"
      },
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvHT4MbkeyiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "device = torch.device('cuda')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcy9EJ7UfFa8",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We then replace the initial weights of the embedding layer with the pre-trained embeddings.\n",
        "\n",
        "Note: this should always be done on the weight.data and not the weight!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1P5ANv8fEY6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "0b84dc91-991e-4359-989d-70b4c3d85103"
      },
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.8330,  1.7377,  0.1831,  ..., -0.7654, -0.3746, -0.7757],\n",
              "        [ 0.9665,  0.0843, -2.0277,  ...,  0.1333, -2.1372, -0.8860],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [ 0.5601, -0.2589,  0.7380,  ..., -1.3936,  0.5910,  1.0687],\n",
              "        [-2.4713, -1.6929,  0.7839,  ...,  1.0108, -0.6083,  1.0693],\n",
              "        [-0.7064, -0.1308, -0.0430,  ..., -1.0220,  0.7037, -0.3897]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7nVfyKdfuBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LR(nn.Module):\n",
        "    def __init__(self, vocab, input_dim, embedding_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(vocab.vectors)\n",
        "        # self.embedding.weight.requires_grad = False\n",
        "        \n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "       \n",
        "        embedded = self.embedding(text)     \n",
        "\n",
        "        hidden = torch.mean(embedded, dim=0)\n",
        "        \n",
        "        return self.fc(hidden)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2fgdLTMf6pN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = LR(TEXT.vocab, INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkAv7s2GfVKX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "8075e378-be4d-48a7-f971-535abd5903bb"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'lr-model_with_pre_trained_wv.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.694 | Train Acc: 51.64%\n",
            "\t Val. Loss: 0.673 |  Val. Acc: 72.08%\n",
            "Epoch: 02 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.683 | Train Acc: 59.66%\n",
            "\t Val. Loss: 0.631 |  Val. Acc: 75.42%\n",
            "Epoch: 03 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.665 | Train Acc: 68.73%\n",
            "\t Val. Loss: 0.569 |  Val. Acc: 76.11%\n",
            "Epoch: 04 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.636 | Train Acc: 73.26%\n",
            "\t Val. Loss: 0.509 |  Val. Acc: 77.18%\n",
            "Epoch: 05 | Epoch Time: 0m 3s\n",
            "\tTrain Loss: 0.599 | Train Acc: 77.67%\n",
            "\t Val. Loss: 0.470 |  Val. Acc: 78.52%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM7LofUFgoEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ea71e01-0768-4902-df71-ca6dbc69a793"
      },
      "source": [
        "model.load_state_dict(torch.load('lr-model_with_pre_trained_wv.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.482 | Test Acc: 77.12%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}